{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG4G3ZvR8Fe61rQ9F3hbln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/8.Machine_Learning/descriptores/a.descenso_gradiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algortimo Descenso de gradiente\n",
        "\n",
        "El método del descenso del gradiente es un algoritmo de optimización que permite converger hacia el valor mínimo de una función mediante un proceso iterativo. En aprendizaje automático básicamente se utiliza para minimizar una función que mide el error de predicción del modelo en el conjunto de datos. A esta función de error se le suele denominar función de coste e identificar con J(θ), en donde θ hace referencia a los parámetros del modelo.\n",
        "\n",
        "Para identificar el mínimo de la función el método del descenso del gradiente calcula la derivada parcial respecto a cada parámetro en el punto de evaluación. La derivada indica el valor y sentido en que se encuentra el mínimo más próximo. Este puede ser tanto un mínimo local como global, el método no los puede diferenciar. El resultado de la derivada se le resta a cada uno de los parámetros multiplicado por la velocidad de aprendizaje (α). La velocidad de aprendizaje generalmente tiene un valor entre 0 y 1 e indica lo rápido que converge el algoritmo. Es importante notar que es necesario seleccionar un valor adecuado. Un valor demasiado bajo puede provocar que nunca se alcance el mínimo. Por otro lado, un valor lo demasiado alto podría saltarse el mínimo.\n",
        "\n",
        "Fuente: https://www.analyticslane.com/2018/12/21/implementacion-del-metodo-descenso-del-gradiente-en-python/"
      ],
      "metadata": {
        "id": "WFW80Q8yUaon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definimos la funcion y su gradiente\n",
        "\n",
        "f  = lambda X: X[0]**2 + X[1]**2    #Funcion\n",
        "df = lambda X: [2*X[0] , 2*X[1]]    #Gradiente\n",
        " \n",
        "df([1,2])"
      ],
      "metadata": {
        "id": "u1-oeViFVX53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTbeTnQ-UCxU"
      },
      "outputs": [],
      "source": [
        "from sympy import symbols, cos, sin, exp\n",
        "from sympy.plotting import plot\n",
        "from sympy.plotting import plot3d\n",
        "x,y = symbols('x y')\n",
        "plot3d( x**2 + y**2,\n",
        "       (x,-5,5),(y,-5,5),\n",
        "       title='x**2 + y**2',\n",
        "       size=(10,10))\n",
        "\n",
        "plot3d( sin(x**2) + cos(y**2) ,\n",
        "       (x,-2,2),(y,-2,2),\n",
        "       title='sin(x**2) + cos(y**2) ',\n",
        "       size=(10,10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math                      #Funciones matematicas\n",
        "import matplotlib.pyplot as plt  #Generacion de gráficos \n",
        "import numpy as np               #Tratamiento matriz N-dimensionales  \n",
        "import random\n",
        "\n",
        "#Prepara los datos para dibujar mapa de niveles de Z\n",
        "resolucion = 100 \n",
        "rango=5\n",
        " \n",
        "X=np.linspace(-rango,rango,resolucion) \n",
        "Y=np.linspace(-rango,rango,resolucion)\n",
        "Z=np.zeros((resolucion,resolucion))\n",
        "for ix,x in enumerate(X):\n",
        "  for iy,y in enumerate(Y):\n",
        "    Z[iy,ix] = f([x,y])\n",
        "\n",
        "#Pinta el mapa de niveles de Z\n",
        "plt.contourf(X,Y,Z,resolucion)\n",
        "plt.colorbar() ## barra vertical al lado del dibujo en los ejes de coordenadas\n",
        "\n",
        "#Generamos un punto aleatorio inicial y pintamos de blanco\n",
        "P_ini=[random.uniform(-2,2  ),random.uniform(-2,2 ) ]\n",
        "print(\"Punto Inicial: \",P_ini)\n",
        "plt.plot(P_ini[0],P_ini[1],\"o\",c=\"white\") ## \"o\" indica que es un punto que se dibuja\n",
        "\n",
        "#Tasa de aprendizaje. Fija. Sería más efectivo reducirlo a medida que nos acercamos.\n",
        "TA=.001\n",
        "## Numero de iteraciones\n",
        "num_iter = 10000\n",
        "## Punto inicial \n",
        "P = copy.deepcopy(P_ini)  ## porque P_ini lo vamos a necesitar mas adelante\n",
        "                          ## y no queremos alterar su valor\n",
        "\n",
        "\n",
        "## ALGORITMO DEL DESCENSO DEL GRADIENTE\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "  grad = df(P)\n",
        "  P[0],P[1] = P[0] - TA*grad[0] , P[1] - TA*grad[1]\n",
        "\n",
        "  ##imprimir cada 10 iteraciones y el ultimo\n",
        "  if (i%10==0  or i==num_iter-1): print(\"primer descenso: \",P) \n",
        "  plt.plot(P[0],P[1],\"o\",c=\"red\")\n",
        "\n",
        "#Dibujamos el punto final y pintamos de verde\n",
        "plt.plot(P[0],P[1],\"o\",c=\"green\")\n",
        "plt.show()\n",
        "print(\"Solucion:\" , P , f(P))\n",
        "\n",
        "\n",
        "x,y = symbols('x y')\n",
        "plot3d( x**2 + y**2,\n",
        "       (x,-5,5),(y,-5,5),\n",
        "       title='x**2 + y**2',\n",
        "       size=(10,10))\n"
      ],
      "metadata": {
        "id": "o-A3GoevVDg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}